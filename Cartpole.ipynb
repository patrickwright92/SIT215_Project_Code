{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time # to get the time\n",
    "import math # needed for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs an entire episode calculating how long the pole stayed upright, until it falls\n",
    "def run_episode(env, parameters):\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        #env.render()\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "\n",
    "\n",
    "def get_discrete_state(state):\n",
    "    discrete_state = state/np_array_win_size+ np.array([15,10,1,10])\n",
    "    return tuple(discrete_state.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "the parameters for this test are [-0.77207358  0.19018166 -0.21072037  0.38291471]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "98.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basic single episode implementation, returns reward value based on one set of paramters\n",
    "env = gym.make('CartPole-v1')\n",
    "print(env.action_space.n)\n",
    "###############################################################\n",
    "parameters = np.random.rand(4) * 2 - 1\n",
    "print('the parameters for this test are', parameters)\n",
    "run_episode(env, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations are 9\n",
      "params are [0.43497743 0.62092678 0.96766357 0.6679255 ]\n",
      "reward is 200.0\n"
     ]
    }
   ],
   "source": [
    "#Random Policy Implementation\n",
    "#random search, runs 10,000 iterations, once it finds a parameter set that returns reward of 200, it breaks and returns\n",
    "bestparams = None\n",
    "bestreward = 0\n",
    "for _ in range(10000):\n",
    "    parameters = np.random.rand(4) * 2 - 1\n",
    "    reward = run_episode(env,parameters)\n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        bestparams = parameters\n",
    "        # considered solved if the agent lasts 200 timesteps\n",
    "        if reward == 200:\n",
    "            break\n",
    "print('iterations are', _)\n",
    "print('params are', parameters)\n",
    "print('reward is', reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining variables for Q-Learning\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 60000\n",
    "total = 0\n",
    "total_reward = 0\n",
    "prior_reward = 0\n",
    "\n",
    "Observation = [30, 30, 50, 50]\n",
    "np_array_win_size = np.array([0.25, 0.25, 0.01, 0.1])\n",
    "\n",
    "epsilon = 1\n",
    "\n",
    "epsilon_decay_value = 0.99995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 50, 50, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q Table setup\n",
    "q_table = np.random.uniform(low=0, high=1, size=(Observation + [env.action_space.n]))\n",
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Time Average: 0.0005395569801330566\n",
      "Mean Reward: 0.02\n",
      "Time Average: 0.0005635209083557129\n",
      "Mean Reward: 22.156\n",
      "Episode: 2000\n",
      "Time Average: 0.0009028017520904541\n",
      "Mean Reward: 22.046\n",
      "Time Average: 0.0005565083026885986\n",
      "Mean Reward: 21.999\n",
      "Episode: 4000\n",
      "Time Average: 0.0007101035118103027\n",
      "Mean Reward: 22.175\n",
      "Time Average: 0.0005814833641052246\n",
      "Mean Reward: 22.939\n",
      "Episode: 6000\n",
      "Time Average: 0.0007001378536224365\n",
      "Mean Reward: 22.377\n",
      "Time Average: 0.0005475616455078125\n",
      "Mean Reward: 21.942\n",
      "Episode: 8000\n",
      "Time Average: 0.0010322530269622802\n",
      "Mean Reward: 22.868\n",
      "Time Average: 0.000545264482498169\n",
      "Mean Reward: 21.7\n",
      "Episode: 10000\n",
      "Time Average: 0.0008856346607208252\n",
      "Mean Reward: 21.902\n",
      "Epsilon: 0.9753093024395111\n",
      "Time Average: 0.0005724875926971435\n",
      "Mean Reward: 22.421\n",
      "Epsilon: 0.9277417467531685\n",
      "Episode: 12000\n",
      "Epsilon: 0.9048351558698463\n",
      "Time Average: 0.0011609878540039063\n",
      "Mean Reward: 23.762\n",
      "Epsilon: 0.8824941446941661\n",
      "Time Average: 0.0006390032768249512\n",
      "Mean Reward: 24.884\n",
      "Epsilon: 0.8394533480303666\n",
      "Episode: 14000\n",
      "Epsilon: 0.818726659298009\n",
      "Time Average: 0.0011422395706176757\n",
      "Mean Reward: 26.622\n",
      "Epsilon: 0.7985117269685725\n",
      "Epsilon: 0.7787959154194878\n",
      "Time Average: 0.0007490851879119873\n",
      "Mean Reward: 29.342\n",
      "Epsilon: 0.7595669010105212\n",
      "Episode: 16000\n",
      "Epsilon: 0.7408126643807126\n",
      "Time Average: 0.0017177975177764893\n",
      "Mean Reward: 32.963\n",
      "Time Average: 0.0008698539733886718\n",
      "Mean Reward: 34.303\n",
      "Episode: 18000\n",
      "Epsilon: 0.6703133426452782\n",
      "Time Average: 0.0018792085647583007\n",
      "Mean Reward: 36.913\n",
      "Epsilon: 0.6537628386312633\n",
      "Epsilon: 0.6376209781063321\n",
      "Time Average: 0.0010114171504974365\n",
      "Mean Reward: 39.703\n",
      "Episode: 20000\n",
      "Time Average: 0.001736581563949585\n",
      "Mean Reward: 43.306\n",
      "Time Average: 0.001251685619354248\n",
      "Mean Reward: 49.294\n",
      "Epsilon: 0.5626967797130051\n",
      "Episode: 22000\n",
      "Time Average: 0.0016615231037139893\n",
      "Mean Reward: 50.091\n",
      "Epsilon: 0.5352530648457575\n",
      "Time Average: 0.0013693647384643555\n",
      "Mean Reward: 53.985\n",
      "Epsilon: 0.5091478283790776\n",
      "Episode: 24000\n",
      "Time Average: 0.0024943158626556396\n",
      "Mean Reward: 60.854\n",
      "Epsilon: 0.484315790359524\n",
      "Epsilon: 0.47235769565598784\n",
      "Time Average: 0.001686493158340454\n",
      "Mean Reward: 65.707\n",
      "Episode: 26000\n",
      "Time Average: 0.002511404752731323\n",
      "Mean Reward: 69.334\n",
      "Epsilon: 0.4274058491752072\n",
      "Time Average: 0.001916968584060669\n",
      "Mean Reward: 75.901\n",
      "Episode: 28000\n",
      "Time Average: 0.004681270837783813\n",
      "Mean Reward: 79.382\n",
      "Epsilon: 0.396522249086328\n",
      "Time Average: 0.002114682912826538\n",
      "Mean Reward: 82.878\n",
      "Epsilon: 0.3771831593051582\n",
      "Episode: 30000\n",
      "Epsilon: 0.3678702439938449\n",
      "Time Average: 0.005600359678268433\n",
      "Mean Reward: 89.477\n",
      "Time Average: 0.0023577816486358644\n",
      "Mean Reward: 94.32\n",
      "Episode: 32000\n",
      "Time Average: 0.004042798042297363\n",
      "Mean Reward: 100.878\n",
      "Epsilon: 0.32464333633178233\n",
      "Epsilon: 0.31662766589938623\n",
      "Time Average: 0.002699817419052124\n",
      "Mean Reward: 106.586\n",
      "Epsilon: 0.30880990796138097\n",
      "Episode: 34000\n",
      "Epsilon: 0.3011851759202241\n",
      "Time Average: 0.007314593553543091\n",
      "Mean Reward: 115.928\n",
      "Epsilon: 0.29374870383187524\n",
      "Epsilon: 0.28649584342677675\n",
      "Time Average: 0.0031008257865905763\n",
      "Mean Reward: 123.022\n",
      "Epsilon: 0.27942206120438906\n",
      "Episode: 36000\n",
      "Time Average: 0.005066551923751831\n",
      "Mean Reward: 130.744\n",
      "Time Average: 0.00346476674079895\n",
      "Mean Reward: 137.931\n",
      "Epsilon: 0.2528309043033471\n",
      "Episode: 38000\n",
      "Time Average: 0.00443515682220459\n",
      "Mean Reward: 125.361\n",
      "Epsilon: 0.23456178479157042\n",
      "Time Average: 0.0033342175483703613\n",
      "Mean Reward: 132.641\n",
      "Epsilon: 0.22877029070403326\n",
      "Episode: 40000\n",
      "Time Average: 0.004198913335800171\n",
      "Mean Reward: 148.573\n",
      "Time Average: 0.003904806137084961\n",
      "Mean Reward: 156.559\n",
      "Episode: 42000\n",
      "Time Average: 0.005260289430618286\n",
      "Mean Reward: 154.771\n",
      "Epsilon: 0.192041986461381\n",
      "Time Average: 0.004479150533676148\n",
      "Mean Reward: 180.613\n",
      "Episode: 44000\n",
      "Time Average: 0.006936776161193848\n",
      "Mean Reward: 173.035\n",
      "Epsilon: 0.17816536796962992\n",
      "Epsilon: 0.17376634075333858\n",
      "Time Average: 0.004593761682510376\n",
      "Mean Reward: 182.277\n",
      "Episode: 46000\n",
      "Epsilon: 0.1652914496910655\n",
      "Time Average: 0.007690016269683838\n",
      "Mean Reward: 183.616\n",
      "Epsilon: 0.15722989402047993\n",
      "Time Average: 0.004806158781051636\n",
      "Mean Reward: 192.765\n",
      "Epsilon: 0.15334777825975254\n",
      "Episode: 48000\n",
      "Time Average: 0.006175177574157715\n",
      "Mean Reward: 194.636\n",
      "Epsilon: 0.14586873652037563\n",
      "Epsilon: 0.1422671356634204\n",
      "Time Average: 0.0051133625507354735\n",
      "Mean Reward: 204.629\n",
      "Epsilon: 0.13875446084395782\n",
      "Episode: 50000\n",
      "Epsilon: 0.13532851641609098\n",
      "Time Average: 0.008050801277160645\n",
      "Mean Reward: 191.338\n",
      "Epsilon: 0.12872830587316755\n",
      "Time Average: 0.005439592361450195\n",
      "Mean Reward: 216.223\n",
      "Episode: 52000\n",
      "Epsilon: 0.12244999924498873\n",
      "Time Average: 0.008777559995651244\n",
      "Mean Reward: 227.82\n",
      "Epsilon: 0.11942662334734862\n",
      "Epsilon: 0.11647789670960881\n",
      "Time Average: 0.004981656789779663\n",
      "Mean Reward: 201.097\n",
      "Epsilon: 0.11360197618947\n",
      "Episode: 54000\n",
      "Epsilon: 0.11079706415310193\n",
      "Time Average: 0.009001160860061646\n",
      "Mean Reward: 191.145\n",
      "Epsilon: 0.10806140735150761\n",
      "Time Average: 0.005389590978622436\n",
      "Mean Reward: 214.026\n",
      "Epsilon: 0.10279106183252165\n",
      "Episode: 56000\n",
      "Time Average: 0.007406050205230713\n",
      "Mean Reward: 205.805\n",
      "Time Average: 0.005554151058197021\n",
      "Mean Reward: 223.997\n",
      "Epsilon: 0.09300896645525675\n",
      "Episode: 58000\n",
      "Time Average: 0.013962931394577027\n",
      "Mean Reward: 232.138\n",
      "Time Average: 0.0053357620239257815\n",
      "Mean Reward: 216.167\n",
      "Episode: 60000\n",
      "Time Average: 0.007465029954910278\n",
      "Mean Reward: 190.468\n"
     ]
    }
   ],
   "source": [
    "for episode in range(EPISODES + 1): #go through the episodes\n",
    "    t0 = time.time() #set the initial time\n",
    "    discrete_state = get_discrete_state(env.reset()) #get the discrete start for the restarted environment \n",
    "    done = False\n",
    "    episode_reward = 0 #reward starts as 0 for each episode\n",
    "\n",
    "    if episode % 2000 == 0: \n",
    "        print(\"Episode: \" + str(episode))\n",
    "    while not done: \n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.argmax(q_table[discrete_state]) #take cordinated action\n",
    "        else:\n",
    "            action = np.random.randint(0, env.action_space.n) #do a random action\n",
    "        new_state, reward, done, _ = env.step(action) #step action to get new states, reward, and the \"done\" status.\n",
    "        episode_reward += reward #add the reward\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        if episode % 2000 == 0: #render\n",
    "            env.render()\n",
    "        if not done: #update q-table\n",
    "            max_future_q = np.max(q_table[new_discrete_state])\n",
    "            current_q = q_table[discrete_state + (action,)]\n",
    "            new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "            q_table[discrete_state + (action,)] = new_q\n",
    "        discrete_state = new_discrete_state\n",
    "    if epsilon > 0.05: #epsilon modification\n",
    "        if episode_reward > prior_reward and episode > 10000:\n",
    "            epsilon = math.pow(epsilon_decay_value, episode - 10000)\n",
    "            if episode % 500 == 0:\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "    t1 = time.time() #episode has finished\n",
    "    episode_total = t1 - t0 #episode total time\n",
    "    total = total + episode_total\n",
    "    total_reward += episode_reward #episode total reward\n",
    "    prior_reward = episode_reward\n",
    "    if episode % 1000 == 0: #every 1000 episodes print the average time and the average reward\n",
    "        mean = total / 1000\n",
    "        print(\"Time Average: \" + str(mean))\n",
    "        total = 0\n",
    "        mean_reward = total_reward / 1000\n",
    "        print(\"Mean Reward: \" + str(mean_reward))\n",
    "        total_reward = 0\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
